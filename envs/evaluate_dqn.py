import torch
import os
import sys
import torch.nn as nn
import numpy as np
from collections import deque, namedtuple
import random
from time import time
import time as time_module

# ê²½ë¡œ ì„¤ì •ì„ ìœ„í•´ ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€ (train_dqn.pyì™€ ë™ì¼)
# [ì£¼ì˜]: ì‹¤ì œ íŒŒì¼ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”.
# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# QWOPEnv ë° ACTIONSë¥¼ ì„í¬íŠ¸ (qwop_env.py íŒŒì¼ì— ë”°ë¼ ê²½ë¡œ ìˆ˜ì • í•„ìš”)
from qwop_env import QWOPEnv, ACTIONS


# ----------------------------------------------------
# 1. Q-Network ëª¨ë¸ ì •ì˜ (QWOP_QNetwork)
#    - train_dqn.py íŒŒì¼ì˜ í´ë˜ìŠ¤ì™€ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.
# ----------------------------------------------------
# (QWOP_QNetwork, _get_conv_output, Transition, ReplayBuffer í´ë˜ìŠ¤ëŠ”
#  train_dqn.py íŒŒì¼ì—ì„œ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì—¬ ì—¬ê¸°ì— ë¶™ì—¬ë„£ê±°ë‚˜,
#  ë³„ë„ì˜ model.py íŒŒì¼ì—ì„œ import í•´ì•¼ í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ ìƒëµí•©ë‹ˆë‹¤.)

class QWOP_QNetwork(nn.Module):
    """
    QWOP ì´ë¯¸ì§€ ìƒíƒœë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ CNN ëª¨ë¸.
    ì…ë ¥: (Batch_Size, Frame_Stack, Height, Width)
    """

    def __init__(self, state_shape, action_size):
        super(QWOP_QNetwork, self).__init__()
        # ìƒíƒœ ì°¨ì›: (frame_stack, H, W) -> (4, 250, 250) (ëŒ€ëµ)
        C, H, W = state_shape

        # QWOP ì´ë¯¸ì§€ëŠ” í‘ë°±ì´ë¯€ë¡œ (C=frame_stack)
        self.conv = nn.Sequential(
            # ì…ë ¥ ì±„ë„=í”„ë ˆì„ ìŠ¤íƒ(ì˜ˆ: 4), ì¶œë ¥ ì±„ë„=32, ì»¤ë„ í¬ê¸°=8, ìŠ¤íŠ¸ë¼ì´ë“œ=4
            nn.Conv2d(C, 32, kernel_size=8, stride=4),
            nn.ReLU(),
            # ì¶œë ¥ ì±„ë„=64, ì»¤ë„ í¬ê¸°=4, ìŠ¤íŠ¸ë¼ì´ë“œ=2
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            # ì¶œë ¥ ì±„ë„=64, ì»¤ë„ í¬ê¸°=3, ìŠ¤íŠ¸ë¼ì´ë“œ=1
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU()
        )

        # Conv ë ˆì´ì–´ì˜ ì¶œë ¥ ì°¨ì› ê³„ì‚° (QWOP í™˜ê²½ì˜ ì¶•ì†Œëœ í¬ê¸°ì— ë”°ë¼ ë‹¬ë¼ì§)
        # ì˜ˆì‹œ: (250x250) -> 8x8 (ëŒ€ëµì ì¸ ê°’)
        # ì‹¤ì œ í™˜ê²½ì˜ ì¶œë ¥ í¬ê¸°ë¥¼ ê³„ì‚°í•˜ì—¬ ì •í™•í•œ `fc_input_size`ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨.
        # ì—¬ê¸°ì„œëŠ” ì„ì‹œ ê°’ 1600ì„ ì‚¬ìš©í•˜ê±°ë‚˜, ë™ì ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ë¡œì§ì´ í•„ìš”í•©ë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” Flatten í›„ í¬ê¸°ë¥¼ ë¯¸ë¦¬ ì •ì˜í–ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
        fc_input_size = self._get_conv_output((C, H, W))

        self.fc = nn.Sequential(
            nn.Linear(fc_input_size, 512),
            nn.ReLU(),
            nn.Linear(512, action_size)  # ì¶œë ¥ì€ 16ê°œ í–‰ë™ì— ëŒ€í•œ Q-ê°’
        )

    # Conv ë ˆì´ì–´ ì¶œë ¥ í¬ê¸° ê³„ì‚°ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜
    def _get_conv_output(self, shape):
        # ë”ë¯¸ ë°ì´í„°ë¡œ í¬ê¸° ê³„ì‚° (ì •í™•í•œ êµ¬í˜„ì„ ìœ„í•´ í•„ìš”)
        input = torch.rand(1, *shape)
        output_feat = self.conv(input)
        # Flattenëœ ë²¡í„°ì˜ í¬ê¸°ë¥¼ ë°˜í™˜
        return int(np.prod(output_feat.size()[1:]))

    def forward(self, x):
        # QWOP í™˜ê²½ì˜ ìƒíƒœëŠ” (T, H, W) í˜•íƒœì˜ NumPy ë°°ì—´ì…ë‹ˆë‹¤.
        # Pytorch CNN ì…ë ¥ì€ (N, C, H, W)ì—¬ì•¼ í•©ë‹ˆë‹¤.
        # N=Batch Size, C=Channels (Frame Stack)
        x = self.conv(x / 255.0)  # í”½ì…€ ê°’ì„ [0, 1]ë¡œ ì •ê·œí™”
        x = x.reshape(x.size(0), -1)  # Flatten
        return self.fc(x)

        pass



Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))


class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        """ìƒˆë¡œìš´ ê²½í—˜ì„ ë²„í¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        self.buffer.append(Transition(state, action, reward, next_state, done))

    def sample(self, batch_size):
        """ë²„í¼ì—ì„œ ë¬´ì‘ìœ„ë¡œ ë°°ì¹˜ í¬ê¸°ë§Œí¼ì˜ ê²½í—˜ì„ ìƒ˜í”Œë§í•©ë‹ˆë‹¤."""
        experiences = random.sample(self.buffer, batch_size)

        # NumPy ë°°ì—´ ê²½í—˜ì„ Pytorch í…ì„œë¡œ ë³€í™˜
        states = torch.as_tensor(np.array([e.state for e in experiences]), dtype=torch.float)
        actions = torch.as_tensor(np.array([e.action for e in experiences]), dtype=torch.long).unsqueeze(-1)
        rewards = torch.as_tensor(np.array([e.reward for e in experiences]), dtype=torch.float).unsqueeze(-1)
        next_states = torch.as_tensor(np.array([e.next_state for e in experiences]), dtype=torch.float)
        dones = torch.as_tensor(np.array([e.done for e in experiences]), dtype=torch.float).unsqueeze(-1)

        return states, actions, rewards, next_states, dones

    def __len__(self):
        return len(self.buffer)


# ----------------------------------------------------
# 2. í‰ê°€ ì—ì´ì „íŠ¸ (Evaluation Agent)
# ----------------------------------------------------
class EvaluationAgent:
    def __init__(self, env, state_shape, action_size, model_path):
        self.env = env
        self.action_size = action_size
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ë° ì €ì¥ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
        self.policy_net = QWOP_QNetwork(state_shape, action_size).to(self.device)
        self.load_model(model_path)
        self.policy_net.eval()  # í‰ê°€ ëª¨ë“œ ì„¤ì • (Dropout, BatchNorm ë¹„í™œì„±í™”)

    def load_model(self, path):
        """ì €ì¥ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        if not os.path.exists(path):
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì´ ì§€ì •ëœ ê²½ë¡œì— ì—†ìŠµë‹ˆë‹¤: {path}")

        # load_state_dictë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ ë¡œë“œ
        self.policy_net.load_state_dict(torch.load(path, map_location=self.device))
        print(f"âœ… ëª¨ë¸ ê°€ì¤‘ì¹˜ '{path}' ë¡œë“œ ì™„ë£Œ.")

    def select_action(self, state):
        """ìˆœìˆ˜í•œ íƒìš• ì •ì±… (Greedy Policy)ìœ¼ë¡œ í–‰ë™ì„ ì„ íƒí•©ë‹ˆë‹¤ (epsilon=0)."""
        with torch.no_grad():
            # ìƒíƒœë¥¼ í…ì„œë¡œ ë³€í™˜í•˜ê³  ë°°ì¹˜ ì°¨ì› ì¶”ê°€: (T, H, W) -> (1, T, H, W)
            state_tensor = torch.as_tensor(state, dtype=torch.float).unsqueeze(0).to(self.device)

            # ë„¤íŠ¸ì›Œí¬ ì‹¤í–‰ ë° ìµœëŒ€ Q-ê°’ì„ ê°€ì§„ í–‰ë™ ì„ íƒ
            q_values = self.policy_net(state_tensor)
            return q_values.argmax(dim=1).item()

    def evaluate(self, num_episodes):
        """ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ QWOPë¥¼ í”Œë ˆì´í•˜ë©° ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤."""

        print("\n==================== í‰ê°€ ì‹œì‘ ====================")

        total_steps = 0
        total_distance = 0.0

        for episode in range(num_episodes):
            state = self.env.reset()
            episode_reward = 0.0
            episode_steps = 0

            while True:
                # 1. íƒìš• í–‰ë™ ì„ íƒ
                action = self.select_action(state)

                # 2. í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©
                next_state, reward, done, info = self.env.step(action)

                # 3. ìƒíƒœ ë° ë³´ìƒ ì—…ë°ì´íŠ¸
                state = next_state
                episode_reward += reward
                episode_steps += 1
                total_steps += 1

                dist = info.get('distance', float('nan'))

                if episode_steps % 50 == 0 or done:
                    print(
                        f"Ep {episode + 1:03d} | Step {episode_steps:04d} | Dist: {dist:.2f}m | Action: {action:02d} | Done: {done}")

                if done:
                    final_dist = info.get('distance', 0.0)
                    total_distance += final_dist

                    print(f"ğŸ‰ EPISODE {episode + 1:03d} ì™„ë£Œ! ìµœì¢… ê±°ë¦¬: {final_dist:.2f}m, ì´ ìŠ¤í…: {episode_steps}")
                    break

        avg_distance = total_distance / num_episodes if num_episodes > 0 else 0
        print("\n==================== í‰ê°€ ê²°ê³¼ ====================")
        print(f"ì´ ì—í”¼ì†Œë“œ: {num_episodes}íšŒ")
        print(f"í‰ê·  ë„ë‹¬ ê±°ë¦¬: {avg_distance:.2f}m")
        print("===================================================")


# ----------------------------------------------------
# 3. ì‹¤í–‰ ë¸”ë¡
# ----------------------------------------------------
if __name__ == '__main__':
    MODEL_FILE = "qwop_dqn_policy_net.pth"
    FRAME_STACK = 4
    NUM_EVAL_EPISODES = 10

    # 1. í™˜ê²½ ì´ˆê¸°í™”
    # í‰ê°€ ì‹œì—ë„ í•™ìŠµê³¼ ë™ì¼í•œ frame_stackìœ¼ë¡œ ì´ˆê¸°í™”í•´ì•¼ í•©ë‹ˆë‹¤.
    # debug_ocrì„ Trueë¡œ ì„¤ì •í•˜ë©´ OCR ê²°ê³¼ í™•ì¸ì— ë„ì›€ì´ ë©ë‹ˆë‹¤.
    env = QWOPEnv(frame_stack=FRAME_STACK, debug_ocr=False)

    # 2. ìƒíƒœ ë° í–‰ë™ ê³µê°„ ì •ì˜ (resetì„ í†µí•´ ì‹¤ì œ í¬ê¸° í™•ì¸)
    initial_obs = env.reset()
    STATE_SHAPE = initial_obs.shape
    ACTION_SIZE = len(ACTIONS)

    # 3. í‰ê°€ ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ë° ëª¨ë¸ ë¡œë“œ
    try:
        agent = EvaluationAgent(
            env=env,
            state_shape=STATE_SHAPE,
            action_size=ACTION_SIZE,
            model_path=MODEL_FILE
        )

        # 4. í‰ê°€ ì‹¤í–‰
        agent.evaluate(num_episodes=NUM_EVAL_EPISODES)

    except FileNotFoundError as e:
        print(f"ì˜¤ë¥˜: {e}")
        print("ğŸ’¡ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ê°€ ì‹¤í–‰ëœ ë””ë ‰í† ë¦¬ì™€ í˜„ì¬ ë””ë ‰í† ë¦¬ê°€ ê°™ì€ì§€ í™•ì¸í•˜ê±°ë‚˜, ëª¨ë¸ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    finally:
        # í™˜ê²½ ì •ë¦¬
        env.close()