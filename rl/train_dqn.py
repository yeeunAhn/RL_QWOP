import torch
import os
import sys
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import deque, namedtuple
import random
from time import time
import time as time_module

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼(train_dqn.py)ì´ ìˆëŠ” ë””ë ‰í† ë¦¬(rl/)ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
script_dir = os.path.dirname(os.path.abspath(__file__))
# ë¶€ëª¨ ë””ë ‰í† ë¦¬(í”„ë¡œì íŠ¸ ë£¨íŠ¸, RL_QWOP/)ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
project_root = os.path.dirname(script_dir)
# íŒŒì´ì¬ì´ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¥¼ ì‹œìŠ¤í…œ ê²½ë¡œì— ì¶”ê°€í•©ë‹ˆë‹¤.
sys.path.append(project_root)

# ğŸ’¡ envs í´ë”ì— ìˆëŠ” qwop_envë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from envs.qwop_env import QWOPEnv, ACTIONS


# ----------------------------------------------------
# 1. DQN Q-Network ëª¨ë¸ ì •ì˜ (CNN)
# ----------------------------------------------------
class QWOP_QNetwork(nn.Module):
    """
    QWOP ì´ë¯¸ì§€ ìƒíƒœë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ CNN ëª¨ë¸.
    ì…ë ¥: (Batch_Size, Frame_Stack, Height, Width)
    """

    def __init__(self, state_shape, action_size):
        super(QWOP_QNetwork, self).__init__()
        # ìƒíƒœ ì°¨ì›: (frame_stack, H, W)
        C, H, W = state_shape

        self.conv = nn.Sequential(
            nn.Conv2d(C, 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU()
        )

        # Conv ë ˆì´ì–´ì˜ ì¶œë ¥ ì°¨ì› ë™ì  ê³„ì‚°
        fc_input_size = self._get_conv_output((C, H, W))

        self.fc = nn.Sequential(
            nn.Linear(fc_input_size, 512),
            nn.ReLU(),
            nn.Linear(512, action_size)  # ì¶œë ¥ì€ í–‰ë™ ê°œìˆ˜ì— ëŒ€í•œ Q-ê°’
        )

    def _get_conv_output(self, shape):
        # ë”ë¯¸ ë°ì´í„°ë¡œ í¬ê¸° ê³„ì‚°
        with torch.no_grad():
            input = torch.rand(1, *shape)
            output_feat = self.conv(input)
            # Flattenëœ ë²¡í„°ì˜ í¬ê¸°ë¥¼ ë°˜í™˜
            return int(np.prod(output_feat.size()[1:]))

    def forward(self, x):
        # (N, C, H, W)
        x = self.conv(x / 255.0)  # í”½ì…€ ê°’ì„ [0, 1]ë¡œ ì •ê·œí™”
        x = torch.flatten(x, 1)  # Batch ì°¨ì›(0)ì„ ì œì™¸í•˜ê³  ëª¨ë‘ Flatten
        return self.fc(x)


# ----------------------------------------------------
# 2. ê²½í—˜ ì¬ìƒ ë²„í¼ (Replay Buffer)
# ----------------------------------------------------
Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))


class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        """ìƒˆë¡œìš´ ê²½í—˜ì„ ë²„í¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        self.buffer.append(Transition(state, action, reward, next_state, done))

    def sample(self, batch_size, device):
        """ë²„í¼ì—ì„œ ë¬´ì‘ìœ„ë¡œ ë°°ì¹˜ í¬ê¸°ë§Œí¼ì˜ ê²½í—˜ì„ ìƒ˜í”Œë§í•©ë‹ˆë‹¤."""
        experiences = random.sample(self.buffer, batch_size)

        states = torch.tensor(np.array([e.state for e in experiences]), dtype=torch.float, device=device)
        actions = torch.tensor(np.array([e.action for e in experiences]), dtype=torch.long, device=device).unsqueeze(-1)
        rewards = torch.tensor(np.array([e.reward for e in experiences]), dtype=torch.float, device=device).unsqueeze(
            -1)
        next_states = torch.tensor(np.array([e.next_state for e in experiences]), dtype=torch.float, device=device)
        dones = torch.tensor(np.array([e.done for e in experiences]), dtype=torch.float, device=device).unsqueeze(-1)

        return states, actions, rewards, next_states, dones

    def __len__(self):
        return len(self.buffer)


# ----------------------------------------------------
# 3. DQN ì—ì´ì „íŠ¸ (Agent)
# ----------------------------------------------------
class DQNAgent:
    def __init__(self, env, state_shape, action_size, **kwargs):
        self.env = env
        self.action_size = action_size
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Device: {self.device} ì‚¬ìš©")

        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.GAMMA = kwargs.get('gamma', 0.99)
        self.LR = kwargs.get('lr', 1e-4)
        self.BATCH_SIZE = kwargs.get('batch_size', 32)
        self.TARGET_UPDATE = kwargs.get('target_update', 1000)  # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ ì£¼ê¸°
        self.EPSILON_START = kwargs.get('eps_start', 1.0)
        self.EPSILON_END = kwargs.get('eps_end', 0.01)
        self.EPSILON_DECAY = kwargs.get('eps_decay', 50000)
        self.REPLAY_CAPACITY = kwargs.get('replay_capacity', 50000)
        self.MIN_REPLAY_SIZE = kwargs.get('min_replay_size', 5000)

        # ë„¤íŠ¸ì›Œí¬ ë° ë²„í¼ ì´ˆê¸°í™”
        self.policy_net = QWOP_QNetwork(state_shape, action_size).to(self.device)
        self.target_net = QWOP_QNetwork(state_shape, action_size).to(self.device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()  # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ëŠ” í•™ìŠµí•˜ì§€ ì•ŠìŒ

        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.LR)
        self.memory = ReplayBuffer(self.REPLAY_CAPACITY)
        self.step_count = 0
        self.current_epsilon = self.EPSILON_START  # ğŸ’¡ ì´ˆê¸°í™”

    def select_action(self, state):
        """Epsilon-Greedy ì •ì±…ì— ë”°ë¼ í–‰ë™ì„ ì„ íƒí•©ë‹ˆë‹¤."""
        epsilon = self.EPSILON_END + (self.EPSILON_START - self.EPSILON_END) * \
                  np.exp(-self.step_count / self.EPSILON_DECAY)

        self.current_epsilon = epsilon  # ë¡œê·¸ìš©

        if random.random() < epsilon:
            return random.randrange(self.action_size)  # íƒí—˜ (ëœë¤ í–‰ë™)
        else:
            with torch.no_grad():
                state_tensor = torch.tensor(state, dtype=torch.float, device=self.device).unsqueeze(0)
                q_values = self.policy_net(state_tensor)
                return q_values.argmax(dim=1).item()  # í™œìš© (ìµœëŒ€ Q-ê°’ í–‰ë™)

    def learn(self):
        """ë²„í¼ì—ì„œ ìƒ˜í”Œë§í•˜ì—¬ ì •ì±… ë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤."""
        if len(self.memory) < self.MIN_REPLAY_SIZE:
            return  # ë²„í¼ê°€ ì¶©ë¶„íˆ ì°° ë•Œê¹Œì§€ í•™ìŠµí•˜ì§€ ì•ŠìŒ

        states, actions, rewards, next_states, dones = self.memory.sample(self.BATCH_SIZE, self.device)

        # 1. í˜„ì¬ Q ê°’ Q(s, a) ê³„ì‚° (Policy Network)
        current_q = self.policy_net(states).gather(1, actions)

        # 2. íƒ€ê²Ÿ Q ê°’ $R + \gamma \cdot \max_{a'} Q_{target}(s', a')$ ê³„ì‚°
        with torch.no_grad():
            next_q_target = self.target_net(next_states).max(1)[0].unsqueeze(1)
            target_q = rewards + (1 - dones) * self.GAMMA * next_q_target

        # Huber Lossë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ì‹¤ ê³„ì‚°
        loss = nn.functional.smooth_l1_loss(current_q, target_q)

        # 3. ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)
        self.optimizer.step()

    def update_target_network(self):
        """ì¼ì • ì£¼ê¸°ë¡œ íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ë¥¼ ì •ì±… ë„¤íŠ¸ì›Œí¬ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        if self.step_count % self.TARGET_UPDATE == 0:
            print(f"--- Step {self.step_count}: íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ ---")
            self.target_net.load_state_dict(self.policy_net.state_dict())

    # ğŸ’¡ [ìˆ˜ì •] train í•¨ìˆ˜ì— ìë™ ì €ì¥ì„ ìœ„í•œ ì¸ì ì¶”ê°€
    def train(self, num_episodes, checkpoint_dir, checkpoint_interval=5000):
        """ë©”ì¸ í•™ìŠµ ë£¨í”„"""

        # ğŸ’¡ ìë™ ì €ì¥ì„ ìœ„í•´ í˜„ì¬ ìŠ¤í… ê¸°ì¤€ìœ¼ë¡œ ë§ˆì§€ë§‰ ì €ì¥ ì§€ì  ê³„ì‚°
        last_checkpoint_step = (self.step_count // checkpoint_interval) * checkpoint_interval

        for episode in range(1, num_episodes + 1):
            state = self.env.reset()
            episode_start = time()
            episode_reward = 0.0
            episode_steps = 0

            while True:
                # 1. í–‰ë™ ì„ íƒ
                action = self.select_action(state)

                # 2. í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©
                next_state, reward, done, info = self.env.step(action)

                # 3. ê²½í—˜ ë²„í¼ì— ì €ì¥
                self.memory.push(state, action, reward, next_state, done)

                # 4. ìƒíƒœ ë° ë³´ìƒ ì—…ë°ì´íŠ¸
                state = next_state
                episode_reward += reward
                self.step_count += 1
                episode_steps += 1

                # 5. í•™ìŠµ (ë²„í¼ê°€ ì°¼ì„ ë•Œë§Œ)
                self.learn()

                # 6. íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ (ì¼ì • ì£¼ê¸°ë§ˆë‹¤)
                self.update_target_network()
                import time as t
                t.sleep(0.001)

                # ğŸ’¡ [ìˆ˜ì •] 5000 ìŠ¤í…ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥ ë¡œì§
                current_checkpoint_step = (self.step_count // checkpoint_interval) * checkpoint_interval
                if current_checkpoint_step > last_checkpoint_step:
                    last_checkpoint_step = current_checkpoint_step  # ë§ˆì§€ë§‰ ì €ì¥ ì§€ì  ê°±ì‹ 

                    # â­ï¸ [ì¶”ê°€] í˜„ì¬ ì‹œê°„ ê°€ì ¸ì˜¤ê¸° ë° í¬ë§·íŒ…
                    timestamp = time()
                    timestamp_str = time_module.strftime("%Y%m%d_%H%M%S", time_module.localtime(timestamp))

                    # â­ï¸ [ìˆ˜ì •] íŒŒì¼ëª…ì— ì‹œê°„ ë¬¸ìì—´ í¬í•¨
                    filename = f"qwop_checkpoint_{timestamp_str}_steps{last_checkpoint_step}.pth"

                    self.save_model(os.path.join(checkpoint_dir, filename))
                    print(f"\n--- ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ: {filename} ---")
                if done:
                    dist = info.get('distance', float('nan'))
                    dist_str = f"{dist:.2f}m" if not np.isnan(dist) else "N/A"

                    print(
                        f"Ep: {episode:04d} | Total Steps: {self.step_count} "
                        f"| Ep Reward: {episode_reward:.2f} | Ep Steps: {episode_steps} "
                        f"| Distance: {dist_str} | Epsilon: {self.current_epsilon:.3f} "
                        f"| Time: {time() - episode_start:.1f}s"
                    )
                    break

        print("DQN í•™ìŠµ ì™„ë£Œ.")

    def save_model(self, path: str):
        """ì •ì±… ë„¤íŠ¸ì›Œí¬ì˜ ê°€ì¤‘ì¹˜ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        os.makedirs(os.path.dirname(path) or '.', exist_ok=True)
        torch.save(self.policy_net.state_dict(), path)
        print(f"ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ '{path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def load_model(self, path: str):
        """íŒŒì¼ì—ì„œ ì •ì±… ë„¤íŠ¸ì›Œí¬ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        if not os.path.exists(path):
            print(f"ê²½ê³ : ëª¨ë¸ íŒŒì¼ '{path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆ ëª¨ë¸ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
            return False

        try:
            self.policy_net.load_state_dict(torch.load(path, map_location=self.device))
            self.target_net.load_state_dict(self.policy_net.state_dict())  # íƒ€ê²Ÿë„·ë„ ë™ê¸°í™”
            print(f"âœ… ëª¨ë¸ ê°€ì¤‘ì¹˜ '{path}' ë¡œë“œ ì™„ë£Œ.")
            return True
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ìƒˆ ëª¨ë¸ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
            return False


# ----------------------------------------------------
# 4. ì‹¤í–‰
# ----------------------------------------------------

if __name__ == '__main__':

    # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
    CHECKPOINT_DIR = os.path.join(project_root, "checkpoints")
    os.makedirs(CHECKPOINT_DIR, exist_ok=True)

    # QWOP í™˜ê²½ ì´ˆê¸°í™” (frame_stack = 4 ê¶Œì¥)
    FRAME_STACK = 4
    env = QWOPEnv(debug_ocr=False, frame_stack=FRAME_STACK, background_safe=True)

    initial_obs = env.reset()

    # ìƒíƒœ ë° í–‰ë™ ê³µê°„ ì •ì˜
    STATE_SHAPE = initial_obs.shape
    ACTION_SIZE = len(ACTIONS)

    # DQN ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
    agent = DQNAgent(
        env=env,
        state_shape=STATE_SHAPE,
        action_size=ACTION_SIZE,
        lr=1e-4,  # í•™ìŠµë¥ 
        gamma=0.99,  # í• ì¸ ê³„ìˆ˜
        batch_size=32,  # ë°°ì¹˜ í¬ê¸°
        target_update=2000,  # íƒ€ê²Ÿ ì—…ë°ì´íŠ¸ ì£¼ê¸° (ìŠ¤í… ê¸°ì¤€)
        eps_decay=40000,  # Epsilon ê°ì†Œ ì†ë„ (ë” ëŠë¦¬ê²Œ)
        replay_capacity=20000,  # ë¦¬í”Œë ˆì´ ë²„í¼ í¬ê¸°
        min_replay_size=5000  # ìµœì†Œ í•™ìŠµ ì‹œì‘ í¬ê¸°
    )

    # ğŸ’¡ [ìˆ˜ì •] ëª¨ë¸ ë¡œë“œ ë¡œì§ (ì´ì–´í•˜ê¸° ì›í•  ë•Œ ì‚¬ìš©)
    # -------------------------------------------------------------------
    # â­ï¸ Trueë¡œ ë°”ê¾¸ê³  ì•„ë˜ 2ì¤„ ìˆ˜ì •í•˜ë©´ ì´ì–´í•˜ê¸°
    LOAD_MODEL = False
    LOAD_STEP = 10000
    MODEL_TO_LOAD = "checkpoints/qwop_checkpoint_20251119_160841_steps10000.pth"  # â­ï¸ ì´ì–´í•  íŒŒì¼ëª…

    if LOAD_MODEL and agent.load_model(MODEL_TO_LOAD):
        agent.step_count = LOAD_STEP
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ. {LOAD_STEP} ìŠ¤í…ë¶€í„° í•™ìŠµì„ ì¬ê°œí•©ë‹ˆë‹¤.")
    else:
        if LOAD_MODEL:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. 0 ìŠ¤í…ë¶€í„° ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        else:
            print("â„¹ï¸ ìƒˆ ëª¨ë¸ë¡œ 0 ìŠ¤í…ë¶€í„° í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        agent.step_count = 0  # â­ï¸ 0ë¶€í„° ìƒˆë¡œ ì‹œì‘
    # -------------------------------------------------------------------

    # í•™ìŠµ ì‹œì‘
    print(f"DQN í•™ìŠµ ì‹œì‘. ìƒíƒœ í¬ê¸°: {STATE_SHAPE}, í–‰ë™ í¬ê¸°: {ACTION_SIZE}")
    print(f"ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {CHECKPOINT_DIR}")

    try:
        # ğŸ’¡ [ìˆ˜ì •] train í•¨ìˆ˜ì— ìë™ ì €ì¥ ê²½ë¡œì™€ ê°„ê²©(5000) ì „ë‹¬
        agent.train(num_episodes=5000,  # (ë„‰ë„‰í•˜ê²Œ)
                    checkpoint_dir=CHECKPOINT_DIR,
                    checkpoint_interval=5000)  # â­ï¸ 5000 ìŠ¤í…ë§ˆë‹¤ ì €ì¥

        # ëª¨ë“  í•™ìŠµì´ ì™„ë£Œëœ eps_decay=100000í›„, íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        print("\n[í•™ìŠµ ì™„ë£Œ] ëª¨ë“  ì—í”¼ì†Œë“œ í•™ìŠµ ì™„ë£Œ. ìµœì¢… ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.")
        timestamp = time()
        timestamp_str = time_module.strftime("%Y%m%d_%H%M%S", time_module.localtime(timestamp))
        filename = f"qwop_completed_{timestamp_str}_steps{agent.step_count}.pth"
        agent.save_model(os.path.join(CHECKPOINT_DIR, filename))
        print(f"\nâœ… ì „ì²´ í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ì´ {filename} ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except KeyboardInterrupt:
        # Ctrl+C ê°ì§€ ì‹œ ì‹¤í–‰
        timestamp = time()
        timestamp_str = time_module.strftime("%Y%m%d_%H%M%S", time_module.localtime(timestamp))
        filename = f"qwop_interrupted_{timestamp_str}_steps{agent.step_count}.pth"
        print(f"\n[Ctrl+C ê°ì§€] í•™ìŠµ ì¤‘ë‹¨ ìš”ì²­. ëª¨ë¸ì„ {filename}ì— ì €ì¥í•©ë‹ˆë‹¤.")
        agent.save_model(os.path.join(CHECKPOINT_DIR, filename))

    except Exception as e:
        print(f"\n[ì˜¤ë¥˜ ê°ì§€] ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì €ì¥ì„ ì‹œë„í•©ë‹ˆë‹¤.
        timestamp = time()
        timestamp_str = time_module.strftime("%Y%m%d_%H%M%S", time_module.localtime(timestamp))
        filename = f"qwop_error_{timestamp_str}_steps{agent.step_count}.pth"
        print(f"ì˜¤ë¥˜ ë°œìƒ ì „ ëª¨ë¸ì„ {filename}ì— ì €ì¥í•©ë‹ˆë‹¤.")
        agent.save_model(os.path.join(CHECKPOINT_DIR, filename))
        # â­ï¸ ì˜¤ë¥˜ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ë¥¼ ì¶œë ¥í•˜ì—¬ ë””ë²„ê¹… ë•ê¸°
        import traceback

        traceback.print_exc()


    finally:
        # í™˜ê²½ ì¢…ë£Œ (ì •ìƒ ì¢…ë£Œ, Ctrl+C ì¢…ë£Œ, ì˜¤ë¥˜ ì¢…ë£Œ ëª¨ë‘ ì‹¤í–‰ë¨)
        if hasattr(env, 'close'):
            env.close()